
import json
import re
import os
from typing import List, Dict, Any, Optional, Union
import requests
from bs4 import BeautifulSoup
import openai
from dotenv import load_dotenv
from pydantic import BaseModel

import config

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = openai.OpenAI(
    api_key=config.OPENAI_API_KEY,
    base_url=config.MODEL_API_URL
)

# íƒ€ì… ì •ì˜
PropertyValue = Union[str, int, float, bool, None]

class Node(BaseModel):
    id: str
    label: str
    properties: Optional[Dict[str, PropertyValue]] = None

class Relationship(BaseModel):
    type: str
    start_node_id: str
    end_node_id: str
    properties: Optional[Dict[str, PropertyValue]] = None

class GraphResponse(BaseModel):
    nodes: List[Node]
    relationships: List[Relationship]

# LLM ì²˜ë¦¬ìš© í…œí”Œë¦¿
UPDATED_TEMPLATE = """
You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph. Extract the entities (nodes) and specify their type from the following text, but **you MUST select nodes ONLY from the following predefined set** (see the provided NODES list below). Do not create any new nodes or use names that do not exactly match one in the NODES list.

Also extract the relationships between these nodes. Return the result as JSON using the following format:

{
  "nodes": [
    {"id": "N0", "label": "ì¸ê°„", "properties": {"name": "Tanjiro Kamado"}}
  ],
  "relationships": [
    {"type": "FIGHTS", "start_node_id": "N0", "end_node_id": "N13", "properties": {"outcome": "victory"}}
  ]
}

Additional rules:
- Use only nodes from the NODES list. Do not invent or substitute nodes.
- Skip any relationship if one of its entities is not in NODES.
- Only output valid relationships where both endpoints exist in NODES and the direction matches their types.

NODES =
[
  {"id":"N0",  "label":"ì¸ê°„", "properties":{"name":"Tanjiro Kamado"}},
  {"id":"N1",  "label":"ì¸ê°„", "properties":{"name":"Nezuko Kamado"}},
  {"id":"N2",  "label":"ì¸ê°„", "properties":{"name":"Giyu Tomioka"}},
  {"id":"N3",  "label":"ì¸ê°„", "properties":{"name":"Sakonji Urokodaki"}},
  {"id":"N4",  "label":"ì¸ê°„", "properties":{"name":"Sabito"}},
  {"id":"N5",  "label":"ì¸ê°„", "properties":{"name":"Makomo"}},
  {"id":"N6",  "label":"ì¸ê°„", "properties":{"name":"Zenitsu Agatsuma"}},
  {"id":"N7",  "label":"ì¸ê°„", "properties":{"name":"Inosuke Hashibira"}},
  {"id":"N8",  "label":"ì¸ê°„", "properties":{"name":"Kanao Tsuyuri"}},
  {"id":"N9",  "label":"ì¸ê°„", "properties":{"name":"Kyojuro Rengoku"}},
  {"id":"N10", "label":"ì¸ê°„", "properties":{"name":"Kagaya Ubuyashiki"}},
  {"id":"N11", "label":"ì¸ê°„", "properties":{"name":"Shinobu Kocho"}},
  {"id":"N12", "label":"ì¸ê°„", "properties":{"name":"Sanemi Shinazugawa"}},
  {"id":"N13", "label":"ë„ê¹¨ë¹„", "properties":{"name":"Muzan Kibutsuji"}},
  {"id":"N14", "label":"ë„ê¹¨ë¹„", "properties":{"name":"Susamaru"}},
  {"id":"N15", "label":"ë„ê¹¨ë¹„", "properties":{"name":"Yahaba"}},
  {"id":"N16", "label":"ë„ê¹¨ë¹„", "properties":{"name":"Kyogai"}},
  {"id":"N17", "label":"ë„ê¹¨ë¹„", "properties":{"name":"Rui"}},
  {"id":"N18", "label":"ë„ê¹¨ë¹„", "properties":{"name":"Enmu"}}
]
"""

# í•œêµ­ì–´ ë…¸ë“œ ì´ë¦„ ë§¤í•‘
# ë…¸ë“œ ì´ë¦„ í•œê¸€ ë§¤í•‘ (ê·€ì‚´ëŒ€ Â· ë„ê¹¨ë¹„)
KOREAN_NODE_MAP = {
    # ê·€ì‚´ëŒ€ (ê·€ì‚´ëŒ€)
    "Tanjiro Kamado": "ì¹´ë§ˆë„ íƒ„ì§€ë¡œ",
    "Nezuko Kamado": "ì¹´ë§ˆë„ ë„¤ì¦ˆì½”",
    "Giyu Tomioka": "í† ë¯¸ì˜¤ì¹´ ê¸°ìœ ",
    "Sakonji Urokodaki": "ìš°ë¡œì½”ë‹¤í‚¤ ì‚¬ì½˜ì§€",
    "Sabito": "ì‚¬ë¹„í† ",
    "Makomo": "ë§ˆì½”ëª¨",
    "Zenitsu Agatsuma": "ì•„ê°€ì¸ ë§ˆ ì  ì´ì¸ ",
    "Inosuke Hashibira": "í•˜ì‹œë¹„ë¼ ì´ë…¸ìŠ¤ì¼€",
    "Kanao Tsuyuri": "ì¸ ìœ ë¦¬ ì¹´ë‚˜ì˜¤",
    "Kyojuro Rengoku": "ë Œê³ ì¿  ì¿„ì¥¬ë¡œ",
    "Kagaya Ubuyashiki": "ìš°ë¶€ì•¼ì‹œí‚¤ ì¹´ê°€ì•¼",
    "Shinobu Kocho": "ì½”ìµ¸ìš° ì‹œë…¸ë¶€",
    "Sanemi Shinazugawa": "ì‹œë‚˜ì¦ˆê°€ì™€ ì‚¬ë„¤ë¯¸",

    # ë„ê¹¨ë¹„ (ë„ê¹¨ë¹„)
    "Muzan Kibutsuji": "í‚¤ë¶€ì¸ ì§€ ë¬´ì”",
    "Susamaru": "ìŠ¤ì‚¬ë§ˆë£¨",
    "Yahaba": "ì•¼í•˜ë°”",
    "Kyogai": "ì¿„ìš°ê°€ì´",
    "Rui": "ë£¨ì´",
    "Enmu": "ì—”ë¬´",
}


def llm_call_structured(prompt: str, model: str = config.LLM_MODEL) -> GraphResponse:
    """êµ¬ì¡°í™”ëœ ì¶œë ¥ìœ¼ë¡œ OpenAI API í˜¸ì¶œ"""
    resp = client.beta.chat.completions.parse(
        model=model,
        messages=[
            {"role": "user", "content": prompt},
        ],
        response_format=GraphResponse,
    )
    return resp.choices[0].message.parsed

def combine_chunk_graphs(chunk_graphs: list) -> 'GraphResponse':
    """
    ì—¬ëŸ¬ ê°œì˜ GraphResponse ê°ì²´ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
    - ëª¨ë“  ë…¸ë“œì™€ ê´€ê³„(relationship)ë¥¼ ëª¨ìë‹ˆë‹¤.
    - ì¤‘ë³µëœ ë…¸ë“œëŠ” ì œê±°í•˜ê³ , ì²˜ìŒ ë“±ì¥í•œ ë…¸ë“œë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    """
    # 1. ëª¨ë“  chunk_graphì—ì„œ ë…¸ë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤
    all_nodes = []
    for chunk_graph in chunk_graphs:
        for node in chunk_graph.nodes:
            all_nodes.append(node)
    
    # 2. ëª¨ë“  chunk_graphì—ì„œ ê´€ê³„(relationship)ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤
    all_relationships = []
    for chunk_graph in chunk_graphs:
        for relationship in chunk_graph.relationships:
            all_relationships.append(relationship)
    
    # 3. ì¤‘ë³µëœ ë…¸ë“œë¥¼ ì œê±°í•©ë‹ˆë‹¤
    unique_nodes = []
    seen = set()  # ì´ë¯¸ ì¶”ê°€ëœ ë…¸ë“œë¥¼ ê¸°ì–µí•´ë‘˜ ì§‘í•©

    for node in all_nodes:
        # ë…¸ë“œì˜ id, label, propertiesë¥¼ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ í‚¤ë¡œ ë§Œë“­ë‹ˆë‹¤
        node_key = (node.id, node.label, str(node.properties))
        # ì´ë¯¸ ì¶”ê°€ëœ ë…¸ë“œê°€ ì•„ë‹ˆë¼ë©´ unique_nodesì— ì¶”ê°€í•©ë‹ˆë‹¤
        if node_key not in seen:
            unique_nodes.append(node)
            seen.add(node_key)

    # 4. ì¤‘ë³µì´ ì œê±°ëœ ë…¸ë“œë“¤ê³¼ ëª¨ë“  ê´€ê³„ë¥¼ í•©ì³ ìƒˆë¡œìš´ GraphResponseë¥¼ ë§Œë“­ë‹ˆë‹¤
    return GraphResponse(nodes=unique_nodes, relationships=all_relationships)

def fetch_episode(link: str) -> List[dict]:
    """ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ì—í”¼ì†Œë“œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤"""
    season = int(re.search(r"season_(\d+)", link).group(1))
    print(f"Fetching Season {season} from: {link}")
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    response = requests.get(link, headers=headers)
    
    soup = BeautifulSoup(response.text, "html.parser")
    table = soup.select_one("table.wikitable.plainrowheaders.wikiepisodetable")

    episodes = []
    rows = table.select("tr.vevent.module-episode-list-row")

    for i, row in enumerate(rows, start=1):
        synopsis = None
        synopsis_row = row.find_next_sibling("tr", class_="expand-child")
        if synopsis_row:
            synopsis_cell = synopsis_row.select_one("td.description div.shortSummaryText")
            synopsis = synopsis_cell.get_text(strip=True) if synopsis_cell else None

        episodes.append({
            "season": season,
            "episode_in_season": i,
            "synopsis": synopsis,
        })
    
    return episodes

def collect_data() -> List[dict]:
    """ì—¬ëŸ¬ ì‹œì¦Œì—ì„œ ì—í”¼ì†Œë“œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤"""
    print("=== ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ===")
    
    episode_links = [
        "https://en.wikipedia.org/wiki/Demon_Slayer:_Kimetsu_no_Yaiba_season_1",  # ê·€ë©¸ì˜ ì¹¼ë‚  ì‹œì¦Œ 1
        # í•„ìš”ì— ë”°ë¼ ë” ë§ì€ ì‹œì¦Œ ì¶”ê°€:
        # "https://en.wikipedia.org/wiki/Demon_Slayer:_Kimetsu_no_Yaiba_season_2",  # ê·€ë©¸ì˜ ì¹¼ë‚  ì‹œì¦Œ 2
    ]
    
    all_episodes = []
    for link in episode_links:
        try:
            episodes = fetch_episode(link)
            all_episodes.extend(episodes)
        except Exception as e:
            print(f"Error fetching data from {link}: {e}")
            continue
    
    print(f"ì´ {len(all_episodes)}ê°œ ì—í”¼ì†Œë“œ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_episodes

def process_data(episodes: List[dict]) -> GraphResponse:
    """ì—í”¼ì†Œë“œ ë°ì´í„°ë¥¼ ì§€ì‹ ê·¸ë˜í”„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤"""
    print("=== ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ ===")
    
    chunk_graphs: List[GraphResponse] = []
    
    for episode in episodes:
        if not episode.get("synopsis"):
            print(f"ì—í”¼ì†Œë“œ S{episode['season']}E{episode['episode_in_season']:02d}: ì‹œë†‰ì‹œìŠ¤ê°€ ì—†ì–´ ê±´ë„ˆëœ€")
            continue
            
        print(f"ì—í”¼ì†Œë“œ ì²˜ë¦¬ ì¤‘: ì‹œì¦Œ {episode['season']}, ì—í”¼ì†Œë“œ {episode['episode_in_season']}")
        
        try:
            # (1) ë…¸ë“œ í‘œì¤€í™”ë¥¼ ìœ„í•œ ì—…ë°ì´íŠ¸ëœ í…œí”Œë¦¿ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = UPDATED_TEMPLATE + f"\n ì…ë ¥ê°’\n {episode['synopsis']}"
            graph_response = llm_call_structured(prompt)

            # (2) ê´€ê³„ì— ì—í”¼ì†Œë“œ ë²ˆí˜¸ ì¶”ê°€ (ì˜ˆ: S1E01)
            episode_number = f"S{episode['season']}E{episode['episode_in_season']:02d}"

            for relationship in graph_response.relationships:
                if relationship.properties is None:
                    relationship.properties = {}
                relationship.properties["episode_number"] = episode_number
                
            # (3) ë…¸ë“œ ì´ë¦„ì„ í•œêµ­ì–´ë¡œ ë³€í™˜
            for node in graph_response.nodes:
                english_name = node.properties.get("name", "")
                if english_name in KOREAN_NODE_MAP:
                    node.properties["name"] = KOREAN_NODE_MAP[english_name]
            
            chunk_graphs.append(graph_response)
            
        except Exception as e:
            print(f"  - ì—í”¼ì†Œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    if not chunk_graphs:
        raise Exception("ê·¸ë˜í”„ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    print(f"ì´ {len(chunk_graphs)}ê°œ ì—í”¼ì†Œë“œ ì²˜ë¦¬ ì™„ë£Œ")
    return combine_chunk_graphs(chunk_graphs)

def save_output(episodes: List[dict], final_graph: GraphResponse):
    """ì¶œë ¥ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤"""
    print("=== ê²°ê³¼ ì €ì¥ ===")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs("output", exist_ok=True)
    
    # ì›ë³¸ ë°ì´í„° ì €ì¥
    with open("output/1_ì›ë³¸ë°ì´í„°.json", "w", encoding="utf-8") as f:
        json.dump(episodes, f, indent=2, ensure_ascii=False)
    print("ì›ë³¸ ë°ì´í„° ì €ì¥: output/1_ì›ë³¸ë°ì´í„°.json")
    
    # ìµœì¢… ì§€ì‹ ê·¸ë˜í”„ ì €ì¥
    with open("output/ì§€ì‹ê·¸ë˜í”„_ìµœì¢….json", "w", encoding="utf-8") as f:
        json.dump(final_graph.model_dump(), f, ensure_ascii=False, indent=2)
    print("ìµœì¢… ì§€ì‹ê·¸ë˜í”„ ì €ì¥: output/ì§€ì‹ê·¸ë˜í”„_ìµœì¢….json")

def main():
    """ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¡°ìœ¨í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    try:
        # config.pyì—ì„œ ë¡œë“œëœ ê°’ì„ í™•ì¸ (ì„ íƒ ì‚¬í•­)
        # if not config.OPENAI_API_KEY: ...
        
        print("ğŸš€ ì§€ì‹ê·¸ë˜í”„ ìƒì„±ê¸° ì‹œì‘")
        print("=" * 50)
        
        # ë‹¨ê³„ 1: ë°ì´í„° ìˆ˜ì§‘
        episodes = collect_data()
        
        if not episodes:
            raise Exception("ìˆ˜ì§‘ëœ ì—í”¼ì†Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë‹¨ê³„ 2: ë°ì´í„° ì²˜ë¦¬
        final_graph = process_data(episodes)
        
        # ë‹¨ê³„ 3: ì¶œë ¥ ì €ì¥
        save_output(episodes, final_graph)
        
        print("=" * 50)
        print("âœ… ì§€ì‹ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ë…¸ë“œ ìˆ˜: {len(final_graph.nodes)}")
        print(f"ğŸ”— ì´ ê´€ê³„ ìˆ˜: {len(final_graph.relationships)}")
        print("\nìƒì„±ëœ íŒŒì¼:")
        print("- output/1_ì›ë³¸ë°ì´í„°.json")
        print("- output/ì§€ì‹ê·¸ë˜í”„_ìµœì¢….json")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
