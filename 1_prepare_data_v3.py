import json
import re
import os
from typing import List, Dict, Any, Optional, Union
import requests
from bs4 import BeautifulSoup
import openai
from dotenv import load_dotenv
from pydantic import BaseModel, field_validator

import config

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = openai.OpenAI(
    api_key=config.OPENAI_API_KEY,
    base_url=config.MODEL_API_URL
)

# íƒ€ì… ì •ì˜
PropertyValue = Union[str, int, float, bool, None]

# ============================================================
# ìœ íš¨í•œ ë…¸ë“œ ì •ì˜ (ë§ˆìŠ¤í„° ë°ì´í„°)
# ============================================================
VALID_NODES = {
    "N0":  {"label": "ì¸ê°„", "name": "ì¹´ë§ˆë„ íƒ„ì§€ë¡œ"},
    "N1":  {"label": "ì¸ê°„", "name": "ì¹´ë§ˆë„ ë„¤ì¦ˆì½”"},
    "N2":  {"label": "ì¸ê°„", "name": "í† ë¯¸ì˜¤ì¹´ ê¸°ìœ "},
    "N3":  {"label": "ì¸ê°„", "name": "ìš°ë¡œì½”ë‹¤í‚¤ ì‚¬ì½˜ì§€"},
    "N4":  {"label": "ì¸ê°„", "name": "ì‚¬ë¹„í† "},
    "N5":  {"label": "ì¸ê°„", "name": "ë§ˆì½”ëª¨"},
    "N6":  {"label": "ì¸ê°„", "name": "ì•„ê°€ì¸ ë§ˆ ì  ì´ì¸ "},
    "N7":  {"label": "ì¸ê°„", "name": "í•˜ì‹œë¹„ë¼ ì´ë…¸ìŠ¤ì¼€"},
    "N8":  {"label": "ì¸ê°„", "name": "ì¸ ìœ ë¦¬ ì¹´ë‚˜ì˜¤"},
    "N9":  {"label": "ì¸ê°„", "name": "ë Œê³ ì¿  ì¿„ì¥¬ë¡œ"},
    "N10": {"label": "ì¸ê°„", "name": "ìš°ë¶€ì•¼ì‹œí‚¤ ì¹´ê°€ì•¼"},
    "N11": {"label": "ì¸ê°„", "name": "ì½”ìµ¸ìš° ì‹œë…¸ë¶€"},
    "N12": {"label": "ì¸ê°„", "name": "ì‹œë‚˜ì¦ˆê°€ì™€ ì‚¬ë„¤ë¯¸"},
    "N13": {"label": "ë„ê¹¨ë¹„", "name": "í‚¤ë¶€ì¸ ì§€ ë¬´ì”"},
    "N14": {"label": "ë„ê¹¨ë¹„", "name": "ìŠ¤ì‚¬ë§ˆë£¨"},
    "N15": {"label": "ë„ê¹¨ë¹„", "name": "ì•¼í•˜ë°”"},
    "N16": {"label": "ë„ê¹¨ë¹„", "name": "ì¿„ìš°ê°€ì´"},
    "N17": {"label": "ë„ê¹¨ë¹„", "name": "ë£¨ì´"},
    "N18": {"label": "ë„ê¹¨ë¹„", "name": "ì—”ë¬´"},
}

# ìœ íš¨í•œ ê´€ê³„ íƒ€ì… ì •ì˜ (ê°œì„ : ë” êµ¬ì²´ì ì¸ ê´€ê³„ ì¶”ê°€)
VALID_RELATIONSHIP_TYPES = [
    "FIGHTS",           # ì‹¸ì›€
    "PROTECTS",         # ë³´í˜¸
    "TRAINS",           # í›ˆë ¨
    "TRAINS_WITH",      # í•¨ê»˜ í›ˆë ¨
    "KNOWS",            # ì•Œê³  ìˆìŒ
    "FAMILY_OF",        # ê°€ì¡±
    "SIBLING_OF",       # í˜•ì œ/ìë§¤
    "ALLY_OF",          # ë™ë§¹
    "ENEMY_OF",         # ì 
    "DEFEATS",          # ë¬¼ë¦¬ì¹¨
    "SAVES",            # êµ¬í•¨
    "RESCUES",          # êµ¬ì¶œ
    "MEETS",            # ë§Œë‚¨
    "ENCOUNTERS",       # ì¡°ìš°
    "GUIDES",           # ì•ˆë‚´
    "ATTACKS",          # ê³µê²©
    "DEFENDS",          # ë°©ì–´
    "TRANSFORMS",       # ë³€ì‹ 
    "JOINS",            # í•©ë¥˜
    "SUPPORTS",         # ì§€ì›
    "REUNITES_WITH",    # ì¬íšŒ
    "BATTLES",          # ì „íˆ¬
    "HEALS",            # ì¹˜ë£Œ
    "TEACHES",          # ê°€ë¥´ì¹¨
]

# ì˜ì–´ â†’ í•œêµ­ì–´ ì´ë¦„ ë§¤í•‘
ENGLISH_TO_KOREAN_NAME = {
    "Tanjiro Kamado": "ì¹´ë§ˆë„ íƒ„ì§€ë¡œ",
    "Tanjiro": "ì¹´ë§ˆë„ íƒ„ì§€ë¡œ",
    "Nezuko Kamado": "ì¹´ë§ˆë„ ë„¤ì¦ˆì½”",
    "Nezuko": "ì¹´ë§ˆë„ ë„¤ì¦ˆì½”",
    "Giyu Tomioka": "í† ë¯¸ì˜¤ì¹´ ê¸°ìœ ",
    "Giyu": "í† ë¯¸ì˜¤ì¹´ ê¸°ìœ ",
    "Sakonji Urokodaki": "ìš°ë¡œì½”ë‹¤í‚¤ ì‚¬ì½˜ì§€",
    "Sakonji": "ìš°ë¡œì½”ë‹¤í‚¤ ì‚¬ì½˜ì§€",
    "Sabito": "ì‚¬ë¹„í† ",
    "Makomo": "ë§ˆì½”ëª¨",
    "Zenitsu Agatsuma": "ì•„ê°€ì¸ ë§ˆ ì  ì´ì¸ ",
    "Zenitsu": "ì•„ê°€ì¸ ë§ˆ ì  ì´ì¸ ",
    "Inosuke Hashibira": "í•˜ì‹œë¹„ë¼ ì´ë…¸ìŠ¤ì¼€",
    "Inosuke": "í•˜ì‹œë¹„ë¼ ì´ë…¸ìŠ¤ì¼€",
    "Kanao Tsuyuri": "ì¸ ìœ ë¦¬ ì¹´ë‚˜ì˜¤",
    "Kanao": "ì¸ ìœ ë¦¬ ì¹´ë‚˜ì˜¤",
    "Kyojuro Rengoku": "ë Œê³ ì¿  ì¿„ì¥¬ë¡œ",
    "Rengoku": "ë Œê³ ì¿  ì¿„ì¥¬ë¡œ",
    "Kagaya Ubuyashiki": "ìš°ë¶€ì•¼ì‹œí‚¤ ì¹´ê°€ì•¼",
    "Kagaya": "ìš°ë¶€ì•¼ì‹œí‚¤ ì¹´ê°€ì•¼",
    "Shinobu Kocho": "ì½”ìµ¸ìš° ì‹œë…¸ë¶€",
    "Shinobu": "ì½”ìµ¸ìš° ì‹œë…¸ë¶€",
    "Sanemi Shinazugawa": "ì‹œë‚˜ì¦ˆê°€ì™€ ì‚¬ë„¤ë¯¸",
    "Sanemi": "ì‹œë‚˜ì¦ˆê°€ì™€ ì‚¬ë„¤ë¯¸",
    "Muzan Kibutsuji": "í‚¤ë¶€ì¸ ì§€ ë¬´ì”",
    "Muzan": "í‚¤ë¶€ì¸ ì§€ ë¬´ì”",
    "Susamaru": "ìŠ¤ì‚¬ë§ˆë£¨",
    "Yahaba": "ì•¼í•˜ë°”",
    "Kyogai": "ì¿„ìš°ê°€ì´",
    "Rui": "ë£¨ì´",
    "Enmu": "ì—”ë¬´",
}

# ì´ë¦„ â†’ ID ì—­ë§¤í•‘
NAME_TO_ID = {v["name"]: k for k, v in VALID_NODES.items()}
ENGLISH_NAME_TO_ID = {
    eng_name: NAME_TO_ID.get(ENGLISH_TO_KOREAN_NAME[eng_name])
    for eng_name in ENGLISH_TO_KOREAN_NAME
    if ENGLISH_TO_KOREAN_NAME[eng_name] in NAME_TO_ID
}

class Node(BaseModel):
    id: str
    label: str
    properties: Optional[Dict[str, PropertyValue]] = None

class Relationship(BaseModel):
    type: str
    start_node_id: str
    end_node_id: str
    properties: Optional[Dict[str, PropertyValue]] = None

class GraphResponse(BaseModel):
    nodes: List[Node]
    relationships: List[Relationship]

# ============================================================
# ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (Phase 1 ê°œì„ )
# ============================================================
EXTRACTION_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì• ë‹ˆë©”ì´ì…˜ "ê·€ë©¸ì˜ ì¹¼ë‚ "ì˜ ì§€ì‹ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì—í”¼ì†Œë“œ ì‹œë†‰ì‹œìŠ¤ì—ì„œ ë“±ì¥ì¸ë¬¼ ê°„ì˜ ê´€ê³„ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”.

## ğŸ¯ í•µì‹¬ ê·œì¹™:
1. **ì—”í‹°í‹° ì¶”ì¶œ**: ì•„ë˜ VALID_NODESì— ì •ì˜ëœ ìºë¦­í„°ë§Œ ì‚¬ìš©
2. **ì •í™•í•œ ì´ë¦„ ë§¤ì¹­**: ì˜ì–´ ì´ë¦„ì€ ENGLISH_TO_KOREAN ë§¤í•‘ ì°¸ì¡°
3. **ë…¸ë“œ ID í˜•ì‹**: ë°˜ë“œì‹œ "N0" ~ "N18" ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©
4. **ê´€ê³„ íƒ€ì…**: VALID_RELATIONSHIP_TYPES ì¤‘ì—ì„œë§Œ ì„ íƒ
5. **ì—í”¼ì†Œë“œ ì •ë³´**: ê° ê´€ê³„ì˜ ë§¥ë½ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª… (50ì ì´ë‚´)
6. **ì¶œë ¥ í˜•ì‹**: ìœ íš¨í•œ JSONë§Œ ë°˜í™˜ (ì„¤ëª…ë¬¸, ì£¼ì„, ì¶”ê°€ í…ìŠ¤íŠ¸ ê¸ˆì§€)

## ğŸ“‹ VALID_NODES (ì‚¬ìš© ê°€ëŠ¥í•œ ìºë¦­í„°):
{valid_nodes_json}

## ğŸ”— VALID_RELATIONSHIP_TYPES:
{valid_relationship_types}

## ğŸŒ ì˜ì–´-í•œêµ­ì–´ ì´ë¦„ ë§¤í•‘:
{english_to_korean_mapping}

## ğŸ“ ì¶œë ¥ JSON í˜•ì‹ (ë°˜ë“œì‹œ ì¤€ìˆ˜):
{{
  "nodes": [
    {{
      "id": "N0",
      "label": "ì¸ê°„",
      "properties": {{"name": "ì¹´ë§ˆë„ íƒ„ì§€ë¡œ"}}
    }}
  ],
  "relationships": [
    {{
      "type": "PROTECTS",
      "start_node_id": "N0",
      "end_node_id": "N1",
      "properties": {{
        "description": "ê´€ê³„ì˜ ìƒì„¸ ë§¥ë½ ì„¤ëª… (ì„ íƒ ì‚¬í•­, ìµœëŒ€ 100ì)",
        "outcome": "ì „íˆ¬/ìƒí˜¸ì‘ìš© ê²°ê³¼ (ì˜ˆ: victory, defeat, ìŠ¹ë¦¬, íŒ¨ë°°)",
        "context": "ê´€ê³„ê°€ í˜•ì„±ëœ ë°°ê²½ (ì˜ˆ: ë„ê¹¨ë¹„ë¡œ ë³€í•œ ë„¤ì¦ˆì½”ë¥¼ ë³´í˜¸í•˜ê¸°ë¡œ ê²°ì‹¬)",
        "action": "ìˆ˜í–‰ëœ êµ¬ì²´ì  í–‰ë™ (ì˜ˆ: kill Father, reaffirm faith)",
        "role": "ê´€ê³„ì—ì„œì˜ ì—­í•  (ì˜ˆ: defender of spirits, ì£¼ìš” ê°€ì¡± êµ¬ì„±ì›)",
        "technique": "ì‚¬ìš©ëœ ê¸°ìˆ /ëŠ¥ë ¥ (ì˜ˆ: Blood Demon Art, Hinokami Kagura)",
        "method": "ì‚¬ìš©ëœ ë°©ë²•/ìˆ˜ë‹¨ (ì˜ˆ: water breathing antidote, night-long ritualistic dance)",
        "effectiveness": "íš¨ê³¼ ì •ë„ (ì˜ˆ: high, medium, low)",
        "duration": "ì§€ì† ê¸°ê°„ (ì˜ˆ: short-term, long-term)",
        "event": "ê´€ë ¨ ì´ë²¤íŠ¸ (ì˜ˆ: train departure, boarding, afterfall)",
        "assistant": "ë³´ì¡°ì ì´ë¦„ (í•´ë‹¹ ì‹œ)",
        "protectees": "ë³´í˜¸ ëŒ€ìƒ (í•´ë‹¹ ì‹œ)",
        "commendation": "ì¹­ì°¬/ì¸ì • ë‚´ìš© (í•´ë‹¹ ì‹œ)",
        "subject": "í›ˆë ¨/êµìœ¡ ëŒ€ìƒ (í•´ë‹¹ ì‹œ)",
        "enemy": "ì ëŒ€ ëŒ€ìƒ ì´ë¦„ (í•´ë‹¹ ì‹œ)",
        "from": "ì¶œë°œ/ì‹œì‘ ì¸ë¬¼",
        "to": "ë„ì°©/ëª©í‘œ ì¸ë¬¼",
        
        "NOTE": "ìœ„ í•„ë“œëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤. ê´€ê³„ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ í•„ë“œë§Œ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”. ë°˜ë“œì‹œ ëª¨ë“  í•„ë“œë¥¼ í¬í•¨í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤."
      }}
    }}
  ]
}}

## ğŸ“‹ Properties ì‚¬ìš© ê°€ì´ë“œ (ê´€ê³„ íƒ€ì…ë³„):

### FIGHTS / BATTLES / DEFEATS:
- **í•„ìˆ˜**: outcome (ìŠ¹ë¦¬/íŒ¨ë°° ê²°ê³¼)
- **ì„ íƒ**: technique (ì‚¬ìš© ê¸°ìˆ ), enemy (ì  ì´ë¦„), description (ì „íˆ¬ ìƒì„¸)

### PROTECTS / SAVES / RESCUES:
- **í•„ìˆ˜**: description ë˜ëŠ” action (ë³´í˜¸/êµ¬ì¶œ í–‰ìœ„)
- **ì„ íƒ**: protectees (ë³´í˜¸ ëŒ€ìƒ), effectiveness (íš¨ê³¼), duration (ì§€ì†ê¸°ê°„), method (ë°©ë²•)

### TRAINS / TEACHES:
- **í•„ìˆ˜**: subject (í›ˆë ¨/êµìœ¡ ëŒ€ìƒ)
- **ì„ íƒ**: assistant (ë³´ì¡°ì), description (í›ˆë ¨ ë‚´ìš©), method (í›ˆë ¨ ë°©ì‹)

### MEETS / ENCOUNTERS:
- **í•„ìˆ˜**: context ë˜ëŠ” event (ë§Œë‚¨ì˜ ë°°ê²½/ì´ë²¤íŠ¸)
- **ì„ íƒ**: outcome (ë§Œë‚¨ì˜ ê²°ê³¼), commendation (í‰ê°€/ë°˜ì‘)

### KNOWS / ALLY_OF / ENEMY_OF:
- **í•„ìˆ˜**: description ë˜ëŠ” context (ê´€ê³„ì˜ ë§¥ë½)
- **ì„ íƒ**: from/to (ê´€ê³„ì˜ ë°©í–¥ì„±), role (ì—­í• )

### ê¸°íƒ€ ê´€ê³„ (JOINS, SUPPORTS, HEALS ë“±):
- **í•„ìˆ˜**: context ë˜ëŠ” description (ê´€ê³„ì˜ ê¸°ë³¸ ë§¥ë½)
- **ì„ íƒ**: ìƒí™©ì— ë§ëŠ” ì¶”ê°€ í•„ë“œ

**ğŸ’¡ ì¤‘ìš”**: ìœ„ ê°€ì´ë“œëŠ” ê¶Œì¥ì‚¬í•­ì…ë‹ˆë‹¤. ì‹œë†‰ì‹œìŠ¤ì— ëª…ì‹œëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ê³ , ì •ë³´ê°€ ì—†ëŠ” í•„ë“œëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

## ì…ë ¥ ì‹œë†‰ì‹œìŠ¤:
ì—í”¼ì†Œë“œ: S{season}E{episode:02d}
{synopsis}

## âš ï¸ ì£¼ì˜ì‚¬í•­:
- ì¶”ì¸¡í•˜ì§€ ë§ê³  ì‹œë†‰ì‹œìŠ¤ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì¶”ì¶œ
- ë™ì¼ ê´€ê³„ ì¤‘ë³µ ì¶”ì¶œ ê¸ˆì§€
- JSON ì™¸ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ í¬í•¨ ê¸ˆì§€

## JSON ì‘ë‹µ:"""

def build_extraction_prompt(synopsis: str, season: int, episode: int) -> str:
    """ê°œì„ ëœ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    valid_nodes_json = json.dumps(
        [{"id": k, "label": v["label"], "name": v["name"]} for k, v in VALID_NODES.items()],
        ensure_ascii=False, indent=2
    )
    valid_relationship_types = ", ".join(VALID_RELATIONSHIP_TYPES)
    english_to_korean_mapping = json.dumps(ENGLISH_TO_KOREAN_NAME, ensure_ascii=False, indent=2)

    return EXTRACTION_PROMPT_TEMPLATE.format(
        valid_nodes_json=valid_nodes_json,
        valid_relationship_types=valid_relationship_types,
        english_to_korean_mapping=english_to_korean_mapping,
        season=season,
        episode=episode,
        synopsis=synopsis
    )

def llm_call_structured(prompt: str, model: str = config.LLM_MODEL, max_retries: int = 3) -> Optional[GraphResponse]:
    """êµ¬ì¡°í™”ëœ ì¶œë ¥ìœ¼ë¡œ OpenAI API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)"""
    for attempt in range(max_retries):
        try:
            resp = client.beta.chat.completions.parse(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                response_format=GraphResponse,
                temperature=0.1,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ temperature
            )
            return resp.choices[0].message.parsed
        except Exception as e:
            print(f"  âš ï¸ API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                return None
            continue
    return None

def normalize_node_id(node_id: str) -> Optional[str]:
    """ë…¸ë“œ ID ì •ê·œí™” - ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ None ë°˜í™˜"""
    if not node_id:
        return None

    # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    cleaned = re.sub(r'[^N0-9]', '', str(node_id).upper())

    # N + ìˆ«ì í˜•ì‹ ì¶”ì¶œ
    match = re.match(r'(N\d+)', cleaned)
    if match:
        normalized = match.group(1)
        if normalized in VALID_NODES:
            return normalized
    return None

def normalize_relationship_type(rel_type: str) -> Optional[str]:
    """ê´€ê³„ íƒ€ì… ì •ê·œí™” (ê°œì„ : ë” ì •êµí•œ ë§¤í•‘)"""
    if not rel_type:
        return "KNOWS"  # ê¸°ë³¸ê°’

    # ëŒ€ë¬¸ìë¡œ ë³€í™˜í•˜ê³  ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    cleaned = re.sub(r'[^A-Z_]', '', str(rel_type).upper())

    # ìœ íš¨í•œ ê´€ê³„ íƒ€ì…ì´ë©´ ë°˜í™˜
    if cleaned in VALID_RELATIONSHIP_TYPES:
        return cleaned

    # ê°œì„ : ë” ì •êµí•œ ìœ ì‚¬ íƒ€ì… ë§¤í•‘
    type_mapping = {
        "FIGHT": "FIGHTS",
        "BATTLE": "BATTLES",
        "PROTECT": "PROTECTS",
        "TRAIN": "TRAINS",
        "KNOW": "KNOWS",
        "FAMILY": "FAMILY_OF",
        "SIBLING": "SIBLING_OF",
        "ALLY": "ALLY_OF",
        "ENEMY": "ENEMY_OF",
        "DEFEAT": "DEFEATS",
        "SAVE": "SAVES",
        "RESCUE": "RESCUES",
        "MEET": "MEETS",
        "ENCOUNTER": "ENCOUNTERS",
        "GUIDE": "GUIDES",
        "ATTACK": "ATTACKS",
        "DEFEND": "DEFENDS",
        "TRANSFORM": "TRANSFORMS",
        "JOIN": "JOINS",
        "SUPPORT": "SUPPORTS",
        "REUNITE": "REUNITES_WITH",
        "HEAL": "HEALS",
        "TEACH": "TEACHES",
        "DEFENDS_FROM": "DEFENDS",
        "SAVES_FROM": "SAVES",
        "PROTECTED_BY": "PROTECTS",
        "TRAINED_BY": "TRAINS",
    }

    return type_mapping.get(cleaned, "KNOWS")

def has_json_artifacts(text: str) -> bool:
    """JSON íŒŒì‹± ì˜¤ë¥˜ í”ì  ê°ì§€"""
    if not text:
        return False

    artifacts = [
        r'}}+,',  # }},
        r'\{[^}]+gave you',  # "gave you" ê°™ì€ í”„ë¡¬í”„íŠ¸ ëˆ„ì¶œ
        r'ì œì™¸í•˜ì„¸ìš”',
        r'í¬í•¨í•˜ì„¸ìš”',
        r'JSON í¬ë§·',
        r'ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì§€',
    ]

    for pattern in artifacts:
        if re.search(pattern, text):
            return True
    return False

def clean_property_value(value: PropertyValue) -> PropertyValue:
    """ì†ì„± ê°’ ì •ì œ (JSON ì•„í‹°íŒ©íŠ¸ ì œê±°)"""
    if not isinstance(value, str):
        return value

    # JSON ì•„í‹°íŒ©íŠ¸ê°€ ìˆìœ¼ë©´ None ë°˜í™˜
    if has_json_artifacts(value):
        return None

    # 200ì ì´ˆê³¼ ì‹œ ì˜ë¼ë‚´ê¸°
    if len(value) > 200:
        return value[:197] + "..."

    return value.strip()

def validate_and_normalize_node(node: Node) -> Optional[Node]:
    """ë…¸ë“œ ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ê·œí™”"""
    # ID ì •ê·œí™”
    normalized_id = normalize_node_id(node.id)
    if not normalized_id:
        return None

    # ë§ˆìŠ¤í„° ë°ì´í„°ì—ì„œ ì˜¬ë°”ë¥¸ ê°’ ê°€ì ¸ì˜¤ê¸°
    master_node = VALID_NODES[normalized_id]

    return Node(
        id=normalized_id,
        label=master_node["label"],
        properties={"name": master_node["name"]}
    )

def validate_and_normalize_relationship(rel: Relationship) -> Optional[Relationship]:
    """ê´€ê³„ ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ê·œí™” (ê°œì„ : ì†ì„± ì •ì œ ì¶”ê°€)"""
    # ë…¸ë“œ ID ì •ê·œí™”
    start_id = normalize_node_id(rel.start_node_id)
    end_id = normalize_node_id(rel.end_node_id)

    if not start_id or not end_id:
        return None

    # ìê¸° ìì‹ ê³¼ì˜ ê´€ê³„ ì œê±°
    if start_id == end_id:
        return None

    # ê´€ê³„ íƒ€ì… ì •ê·œí™”
    rel_type = normalize_relationship_type(rel.type)

    # ì†ì„± ì •ì œ
    cleaned_properties = {}
    if rel.properties:
        for key, value in rel.properties.items():
            cleaned_value = clean_property_value(value)
            if cleaned_value is not None:
                cleaned_properties[key] = cleaned_value

    return Relationship(
        type=rel_type,
        start_node_id=start_id,
        end_node_id=end_id,
        properties=cleaned_properties if cleaned_properties else None
    )

def validate_and_clean_graph(graph: GraphResponse) -> GraphResponse:
    """ê·¸ë˜í”„ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ì œ"""
    valid_nodes = []
    valid_relationships = []
    seen_node_ids = set()
    seen_relationships = set()

    # ë…¸ë“œ ì •ì œ
    for node in graph.nodes:
        normalized = validate_and_normalize_node(node)
        if normalized and normalized.id not in seen_node_ids:
            valid_nodes.append(normalized)
            seen_node_ids.add(normalized.id)

    # ê´€ê³„ ì •ì œ
    for rel in graph.relationships:
        normalized = validate_and_normalize_relationship(rel)
        if normalized:
            # ì¤‘ë³µ ê´€ê³„ ì œê±° (ê°™ì€ íƒ€ì…, ê°™ì€ ë…¸ë“œ ìŒ)
            rel_key = (normalized.type, normalized.start_node_id, normalized.end_node_id)
            if rel_key not in seen_relationships:
                valid_relationships.append(normalized)
                seen_relationships.add(rel_key)

    return GraphResponse(nodes=valid_nodes, relationships=valid_relationships)

def combine_chunk_graphs(chunk_graphs: List[GraphResponse]) -> GraphResponse:
    """ì—¬ëŸ¬ GraphResponseë¥¼ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤."""
    all_nodes = []
    all_relationships = []
    seen_nodes = set()
    seen_relationships = set()

    for chunk_graph in chunk_graphs:
        # ë…¸ë“œ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
        for node in chunk_graph.nodes:
            node_key = node.id
            if node_key not in seen_nodes:
                all_nodes.append(node)
                seen_nodes.add(node_key)

        # ê´€ê³„ ìˆ˜ì§‘ (ê°œì„ : ì—í”¼ì†Œë“œë³„ ë™ì¼ ê´€ê³„ í—ˆìš©)
        for rel in chunk_graph.relationships:
            rel_key = (
                rel.type,
                rel.start_node_id,
                rel.end_node_id,
                rel.properties.get("episode_number") if rel.properties else None
            )
            if rel_key not in seen_relationships:
                all_relationships.append(rel)
                seen_relationships.add(rel_key)

    return GraphResponse(nodes=all_nodes, relationships=all_relationships)

def fetch_episode(link: str) -> List[dict]:
    """ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ì—í”¼ì†Œë“œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤"""
    season = int(re.search(r"season_(\d+)", link).group(1))
    print(f"ğŸ“¥ Season {season} ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘: {link}")
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    response = requests.get(link, headers=headers)

    soup = BeautifulSoup(response.text, "html.parser")
    table = soup.select_one("table.wikitable.plainrowheaders.wikiepisodetable")

    episodes = []
    rows = table.select("tr.vevent.module-episode-list-row")

    for i, row in enumerate(rows, start=1):
        synopsis = None
        synopsis_row = row.find_next_sibling("tr", class_="expand-child")
        if synopsis_row:
            synopsis_cell = synopsis_row.select_one("td.description div.shortSummaryText")
            synopsis = synopsis_cell.get_text(strip=True) if synopsis_cell else None

        episodes.append({
            "season": season,
            "episode_in_season": i,
            "synopsis": synopsis,
        })

    return episodes

def collect_data(use_cache: bool = True) -> List[dict]:
    """ì—¬ëŸ¬ ì‹œì¦Œì—ì„œ ì—í”¼ì†Œë“œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤"""
    print("\n" + "="*60)
    print("ğŸ“š ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print("="*60)

    cache_file = "output/raw_data_v3.json"

    # ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¨¼ì € ì‚¬ìš©
    if use_cache and os.path.exists(cache_file):
        print(f"ğŸ’¾ ìºì‹œ íŒŒì¼ ì‚¬ìš©: {cache_file}")
        with open(cache_file, "r", encoding="utf-8") as f:
            episodes = json.load(f)
        print(f"âœ… ì´ {len(episodes)}ê°œ ì—í”¼ì†Œë“œ ë¡œë“œ ì™„ë£Œ (ìºì‹œ)")
        return episodes

    episode_links = [
        "https://en.wikipedia.org/wiki/Demon_Slayer:_Kimetsu_no_Yaiba_season_1",
        # "https://en.wikipedia.org/wiki/Demon_Slayer:_Kimetsu_no_Yaiba_season_2",  # ê·€ë©¸ì˜ ì¹¼ë‚  ì‹œì¦Œ 2
    ]

    all_episodes = []
    for link in episode_links:
        try:
            episodes = fetch_episode(link)
            all_episodes.extend(episodes)
        except Exception as e:
            print(f"âŒ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({link}): {e}")
            continue

    print(f"âœ… ì´ {len(all_episodes)}ê°œ ì—í”¼ì†Œë“œ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_episodes

def process_data(episodes: List[dict]) -> GraphResponse:
    """ì—í”¼ì†Œë“œ ë°ì´í„°ë¥¼ ì§€ì‹ ê·¸ë˜í”„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ê°œì„ : ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)"""
    print("\n" + "="*60)
    print("ğŸ”„ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
    print("="*60)

    chunk_graphs: List[GraphResponse] = []
    failed_episodes = []

    for episode in episodes:
        if not episode.get("synopsis"):
            print(f"â­ï¸  S{episode['season']}E{episode['episode_in_season']:02d}: ì‹œë†‰ì‹œìŠ¤ ì—†ìŒ - ê±´ë„ˆëœ€")
            continue

        episode_code = f"S{episode['season']}E{episode['episode_in_season']:02d}"
        print(f"\nğŸ¬ ì²˜ë¦¬ ì¤‘: {episode_code}")

        try:
            # (1) ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM í˜¸ì¶œ
            prompt = build_extraction_prompt(
                episode['synopsis'],
                episode['season'],
                episode['episode_in_season']
            )
            graph_response = llm_call_structured(prompt)

            if graph_response is None:
                print(f"  âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨")
                failed_episodes.append(episode_code)
                continue

            # (2) ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ì œ
            graph_response = validate_and_clean_graph(graph_response)

            # (3) ê´€ê³„ì— êµ¬ì¡°í™”ëœ ì—í”¼ì†Œë“œ ì •ë³´ ì¶”ê°€
            for relationship in graph_response.relationships:
                if relationship.properties is None:
                    relationship.properties = {}
                relationship.properties["episode_number"] = episode_code
                relationship.properties["season"] = episode['season']
                relationship.properties["episode"] = episode['episode_in_season']

            chunk_graphs.append(graph_response)
            print(f"  âœ… ì¶”ì¶œ: ë…¸ë“œ {len(graph_response.nodes)}ê°œ, ê´€ê³„ {len(graph_response.relationships)}ê°œ")

        except Exception as e:
            print(f"  âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            failed_episodes.append(episode_code)
            continue

    if not chunk_graphs:
        raise Exception("âŒ ê·¸ë˜í”„ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    print(f"\n{'='*60}")
    print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {len(chunk_graphs)}ê°œ ì—í”¼ì†Œë“œ ì„±ê³µ")
    if failed_episodes:
        print(f"âš ï¸  ì‹¤íŒ¨í•œ ì—í”¼ì†Œë“œ: {', '.join(failed_episodes)}")
    print("="*60)

    return combine_chunk_graphs(chunk_graphs)

def save_output(episodes: List[dict], final_graph: GraphResponse):
    """ì¶œë ¥ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤"""
    print("\n" + "="*60)
    print("ğŸ’¾ ê²°ê³¼ ì €ì¥")
    print("="*60)

    os.makedirs("output", exist_ok=True)

    # ì›ë³¸ ë°ì´í„° ì €ì¥
    with open("output/raw_data_v3.json", "w", encoding="utf-8") as f:
        json.dump(episodes, f, indent=2, ensure_ascii=False)
    print("âœ… ì›ë³¸ ë°ì´í„°: output/raw_data_v3.json")

    # ìµœì¢… ì§€ì‹ ê·¸ë˜í”„ ì €ì¥
    with open("output/knowledge_graph_v3.json", "w", encoding="utf-8") as f:
        json.dump(final_graph.model_dump(), f, ensure_ascii=False, indent=2)
    print("âœ… ìµœì¢… ì§€ì‹ê·¸ë˜í”„: output/knowledge_graph_v3.json")

    # í†µê³„ ì •ë³´ ì €ì¥
    stats = generate_statistics(final_graph)
    with open("output/statistics_v3.json", "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print("âœ… í†µê³„ ì •ë³´: output/statistics_v3.json")

def generate_statistics(graph: GraphResponse) -> Dict[str, Any]:
    """ì§€ì‹ ê·¸ë˜í”„ í†µê³„ ìƒì„±"""
    # ë…¸ë“œë³„ ê´€ê³„ ìˆ˜ ê³„ì‚°
    node_degree = {}
    for rel in graph.relationships:
        node_degree[rel.start_node_id] = node_degree.get(rel.start_node_id, 0) + 1
        node_degree[rel.end_node_id] = node_degree.get(rel.end_node_id, 0) + 1

    # ê´€ê³„ íƒ€ì…ë³„ ë¹ˆë„
    rel_type_count = {}
    for rel in graph.relationships:
        rel_type_count[rel.type] = rel_type_count.get(rel.type, 0) + 1

    # ì—í”¼ì†Œë“œë³„ ê´€ê³„ ìˆ˜
    episode_count = {}
    for rel in graph.relationships:
        if rel.properties and "episode_number" in rel.properties:
            ep = rel.properties["episode_number"]
            episode_count[ep] = episode_count.get(ep, 0) + 1

    return {
        "total_nodes": len(graph.nodes),
        "total_relationships": len(graph.relationships),
        "node_degree": dict(sorted(node_degree.items(), key=lambda x: x[1], reverse=True)),
        "relationship_type_count": dict(sorted(rel_type_count.items(), key=lambda x: x[1], reverse=True)),
        "episode_relationship_count": dict(sorted(episode_count.items())),
    }

def validate_final_output() -> bool:
    """ìµœì¢… ì¶œë ¥ íŒŒì¼ ê²€ì¦ (ê°œì„ : ë” ìƒì„¸í•œ ê²€ì¦)"""
    print("\n" + "="*60)
    print("ğŸ” ìµœì¢… ì¶œë ¥ ê²€ì¦")
    print("="*60)

    try:
        with open("output/knowledge_graph_v3.json", "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return False

    errors = []
    warnings = []

    # ë…¸ë“œ ê²€ì¦
    for node in data.get("nodes", []):
        if node["id"] not in VALID_NODES:
            errors.append(f"ì˜ëª»ëœ ë…¸ë“œ ID: {node['id']}")
        if node["label"] not in ["ì¸ê°„", "ë„ê¹¨ë¹„"]:
            errors.append(f"ì˜ëª»ëœ ë¼ë²¨: {node['label']} (ë…¸ë“œ: {node['id']})")
        if not node.get("properties", {}).get("name"):
            warnings.append(f"ë…¸ë“œ {node['id']}: ì´ë¦„ ì†ì„± ì—†ìŒ")

    # ê´€ê³„ ê²€ì¦
    for i, rel in enumerate(data.get("relationships", [])):
        if rel["start_node_id"] not in VALID_NODES:
            errors.append(f"ê´€ê³„ {i}: ì˜ëª»ëœ ì‹œì‘ ë…¸ë“œ {rel['start_node_id']}")
        if rel["end_node_id"] not in VALID_NODES:
            errors.append(f"ê´€ê³„ {i}: ì˜ëª»ëœ ì¢…ë£Œ ë…¸ë“œ {rel['end_node_id']}")
        if rel["type"] not in VALID_RELATIONSHIP_TYPES:
            errors.append(f"ê´€ê³„ {i}: ì˜ëª»ëœ ê´€ê³„ íƒ€ì… {rel['type']}")

        # ì—í”¼ì†Œë“œ ë²ˆí˜¸ ê²€ì¦
        if rel.get("properties"):
            ep_num = rel["properties"].get("episode_number")
            if ep_num and not re.match(r'S\d+E\d+', ep_num):
                warnings.append(f"ê´€ê³„ {i}: ì˜ëª»ëœ ì—í”¼ì†Œë“œ í˜•ì‹ {ep_num}")

            # JSON ì•„í‹°íŒ©íŠ¸ ê²€ì¦
            for key, value in rel["properties"].items():
                if isinstance(value, str) and has_json_artifacts(value):
                    warnings.append(f"ê´€ê³„ {i}: JSON ì•„í‹°íŒ©íŠ¸ ê°ì§€ in '{key}'")

    # ê²°ê³¼ ì¶œë ¥
    if errors:
        print(f"\nâŒ ê²€ì¦ ì‹¤íŒ¨: {len(errors)}ê°œ ì˜¤ë¥˜ ë°œê²¬")
        for error in errors[:20]:
            print(f"  - {error}")
        if len(errors) > 20:
            print(f"  ... ì™¸ {len(errors) - 20}ê°œ ì˜¤ë¥˜")
        return False

    if warnings:
        print(f"\nâš ï¸  ê²½ê³ : {len(warnings)}ê°œ")
        for warning in warnings[:10]:
            print(f"  - {warning}")

    print(f"\nâœ… ê²€ì¦ í†µê³¼!")
    print(f"  ğŸ“Š ìœ íš¨í•œ ë…¸ë“œ: {len(data.get('nodes', []))}ê°œ")
    print(f"  ğŸ”— ìœ íš¨í•œ ê´€ê³„: {len(data.get('relationships', []))}ê°œ")

    # í†µê³„ ì¶œë ¥
    try:
        with open("output/statistics_v3.json", "r", encoding="utf-8") as f:
            stats = json.load(f)

        print(f"\nğŸ“ˆ ìƒìœ„ ì—°ê²° ë…¸ë“œ:")
        for node_id, count in list(stats["node_degree"].items())[:5]:
            node_name = VALID_NODES[node_id]["name"]
            print(f"  - {node_name} ({node_id}): {count}ê°œ ê´€ê³„")

        print(f"\nğŸ”— ìƒìœ„ ê´€ê³„ íƒ€ì…:")
        for rel_type, count in list(stats["relationship_type_count"].items())[:5]:
            print(f"  - {rel_type}: {count}ê°œ")
    except:
        pass

    return True

def main():
    """ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¡°ìœ¨í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    try:
        print("\n" + "="*60)
        print("ğŸš€ ê°œì„ ëœ ì§€ì‹ê·¸ë˜í”„ ìƒì„±ê¸° v3.0")
        print("="*60)

        # ë‹¨ê³„ 1: ë°ì´í„° ìˆ˜ì§‘
        episodes = collect_data()

        if not episodes:
            raise Exception("âŒ ìˆ˜ì§‘ëœ ì—í”¼ì†Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ë‹¨ê³„ 2: ë°ì´í„° ì²˜ë¦¬
        final_graph = process_data(episodes)

        # ë‹¨ê³„ 3: ì¶œë ¥ ì €ì¥
        save_output(episodes, final_graph)

        # ë‹¨ê³„ 4: ê²€ì¦
        is_valid = validate_final_output()

        print("\n" + "="*60)
        if is_valid:
            print("âœ… ì§€ì‹ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ!")
        else:
            print("âš ï¸  ì§€ì‹ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ (ì¼ë¶€ ê²€ì¦ ì‹¤íŒ¨)")
        print(f"ğŸ“Š ì´ ë…¸ë“œ ìˆ˜: {len(final_graph.nodes)}")
        print(f"ğŸ”— ì´ ê´€ê³„ ìˆ˜: {len(final_graph.relationships)}")
        print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print("  - output/raw_data_v3.json")
        print("  - output/knowledge_graph_v3.json")
        print("  - output/statistics_v3.json")
        print("="*60)

        return 0 if is_valid else 1

    except Exception as e:
        print(f"\nâŒ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())
