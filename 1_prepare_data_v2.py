
import json
import re
import os
from typing import List, Dict, Any, Optional, Union
import requests
from bs4 import BeautifulSoup
import openai
from dotenv import load_dotenv
from pydantic import BaseModel, field_validator

import config

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = openai.OpenAI(
    api_key=config.OPENAI_API_KEY,
    base_url=config.MODEL_API_URL
)

# íƒ€ì… ì •ì˜
PropertyValue = Union[str, int, float, bool, None]

# ============================================================
# ìœ íš¨í•œ ë…¸ë“œ ì •ì˜ (ë§ˆìŠ¤í„° ë°ì´í„°)
# ============================================================
VALID_NODES = {
    "N0":  {"label": "ì¸ê°„", "name": "ì¹´ë§ˆë„ íƒ„ì§€ë¡œ"},
    "N1":  {"label": "ì¸ê°„", "name": "ì¹´ë§ˆë„ ë„¤ì¦ˆì½”"},
    "N2":  {"label": "ì¸ê°„", "name": "í† ë¯¸ì˜¤ì¹´ ê¸°ìœ "},
    "N3":  {"label": "ì¸ê°„", "name": "ìš°ë¡œì½”ë‹¤í‚¤ ì‚¬ì½˜ì§€"},
    "N4":  {"label": "ì¸ê°„", "name": "ì‚¬ë¹„í† "},
    "N5":  {"label": "ì¸ê°„", "name": "ë§ˆì½”ëª¨"},
    "N6":  {"label": "ì¸ê°„", "name": "ì•„ê°€ì¸ ë§ˆ ì  ì´ì¸ "},
    "N7":  {"label": "ì¸ê°„", "name": "í•˜ì‹œë¹„ë¼ ì´ë…¸ìŠ¤ì¼€"},
    "N8":  {"label": "ì¸ê°„", "name": "ì¸ ìœ ë¦¬ ì¹´ë‚˜ì˜¤"},
    "N9":  {"label": "ì¸ê°„", "name": "ë Œê³ ì¿  ì¿„ì¥¬ë¡œ"},
    "N10": {"label": "ì¸ê°„", "name": "ìš°ë¶€ì•¼ì‹œí‚¤ ì¹´ê°€ì•¼"},
    "N11": {"label": "ì¸ê°„", "name": "ì½”ìµ¸ìš° ì‹œë…¸ë¶€"},
    "N12": {"label": "ì¸ê°„", "name": "ì‹œë‚˜ì¦ˆê°€ì™€ ì‚¬ë„¤ë¯¸"},
    "N13": {"label": "ë„ê¹¨ë¹„", "name": "í‚¤ë¶€ì¸ ì§€ ë¬´ì”"},
    "N14": {"label": "ë„ê¹¨ë¹„", "name": "ìŠ¤ì‚¬ë§ˆë£¨"},
    "N15": {"label": "ë„ê¹¨ë¹„", "name": "ì•¼í•˜ë°”"},
    "N16": {"label": "ë„ê¹¨ë¹„", "name": "ì¿„ìš°ê°€ì´"},
    "N17": {"label": "ë„ê¹¨ë¹„", "name": "ë£¨ì´"},
    "N18": {"label": "ë„ê¹¨ë¹„", "name": "ì—”ë¬´"},
}

# ìœ íš¨í•œ ê´€ê³„ íƒ€ì… ì •ì˜
VALID_RELATIONSHIP_TYPES = [
    "FIGHTS",      # ì‹¸ì›€
    "PROTECTS",    # ë³´í˜¸
    "TRAINS",      # í›ˆë ¨
    "KNOWS",       # ì•Œê³  ìˆìŒ
    "FAMILY_OF",   # ê°€ì¡±
    "ALLY_OF",     # ë™ë§¹
    "ENEMY_OF",    # ì 
    "DEFEATS",     # ë¬¼ë¦¬ì¹¨
    "SAVES",       # êµ¬í•¨
    "MEETS",       # ë§Œë‚¨
]

# ì˜ì–´ â†’ í•œêµ­ì–´ ì´ë¦„ ë§¤í•‘
ENGLISH_TO_KOREAN_NAME = {
    "Tanjiro Kamado": "ì¹´ë§ˆë„ íƒ„ì§€ë¡œ",
    "Nezuko Kamado": "ì¹´ë§ˆë„ ë„¤ì¦ˆì½”",
    "Giyu Tomioka": "í† ë¯¸ì˜¤ì¹´ ê¸°ìœ ",
    "Sakonji Urokodaki": "ìš°ë¡œì½”ë‹¤í‚¤ ì‚¬ì½˜ì§€",
    "Sabito": "ì‚¬ë¹„í† ",
    "Makomo": "ë§ˆì½”ëª¨",
    "Zenitsu Agatsuma": "ì•„ê°€ì¸ ë§ˆ ì  ì´ì¸ ",
    "Inosuke Hashibira": "í•˜ì‹œë¹„ë¼ ì´ë…¸ìŠ¤ì¼€",
    "Kanao Tsuyuri": "ì¸ ìœ ë¦¬ ì¹´ë‚˜ì˜¤",
    "Kyojuro Rengoku": "ë Œê³ ì¿  ì¿„ì¥¬ë¡œ",
    "Kagaya Ubuyashiki": "ìš°ë¶€ì•¼ì‹œí‚¤ ì¹´ê°€ì•¼",
    "Shinobu Kocho": "ì½”ìµ¸ìš° ì‹œë…¸ë¶€",
    "Sanemi Shinazugawa": "ì‹œë‚˜ì¦ˆê°€ì™€ ì‚¬ë„¤ë¯¸",
    "Muzan Kibutsuji": "í‚¤ë¶€ì¸ ì§€ ë¬´ì”",
    "Susamaru": "ìŠ¤ì‚¬ë§ˆë£¨",
    "Yahaba": "ì•¼í•˜ë°”",
    "Kyogai": "ì¿„ìš°ê°€ì´",
    "Rui": "ë£¨ì´",
    "Enmu": "ì—”ë¬´",
}

# ì´ë¦„ â†’ ID ì—­ë§¤í•‘
NAME_TO_ID = {v["name"]: k for k, v in VALID_NODES.items()}
ENGLISH_NAME_TO_ID = {
    "Tanjiro Kamado": "N0",
    "Nezuko Kamado": "N1",
    "Giyu Tomioka": "N2",
    "Sakonji Urokodaki": "N3",
    "Sabito": "N4",
    "Makomo": "N5",
    "Zenitsu Agatsuma": "N6",
    "Inosuke Hashibira": "N7",
    "Kanao Tsuyuri": "N8",
    "Kyojuro Rengoku": "N9",
    "Kagaya Ubuyashiki": "N10",
    "Shinobu Kocho": "N11",
    "Sanemi Shinazugawa": "N12",
    "Muzan Kibutsuji": "N13",
    "Susamaru": "N14",
    "Yahaba": "N15",
    "Kyogai": "N16",
    "Rui": "N17",
    "Enmu": "N18",
}


class Node(BaseModel):
    id: str
    label: str
    properties: Optional[Dict[str, PropertyValue]] = None


class Relationship(BaseModel):
    type: str
    start_node_id: str
    end_node_id: str
    properties: Optional[Dict[str, PropertyValue]] = None


class GraphResponse(BaseModel):
    nodes: List[Node]
    relationships: List[Relationship]


# ============================================================
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
# ============================================================
EXTRACTION_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì§€ì‹ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ë“±ì¥ì¸ë¬¼ ê°„ì˜ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

## ì¤‘ìš” ê·œì¹™:
1. ë°˜ë“œì‹œ ì•„ë˜ VALID_NODESì— ì •ì˜ëœ ìºë¦­í„°ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. ë…¸ë“œ IDëŠ” ì •í™•íˆ "N0", "N1", ... "N18" í˜•ì‹ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
3. ê´€ê³„ íƒ€ì…ì€ ë°˜ë“œì‹œ VALID_RELATIONSHIP_TYPES ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
4. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

## VALID_NODES:
{valid_nodes_json}

## VALID_RELATIONSHIP_TYPES:
{valid_relationship_types}

## ì¶œë ¥ í˜•ì‹:
{{
  "nodes": [
    {{"id": "N0", "label": "ì¸ê°„", "properties": {{"name": "ì¹´ë§ˆë„ íƒ„ì§€ë¡œ"}}}}
  ],
  "relationships": [
    {{"type": "FIGHTS", "start_node_id": "N0", "end_node_id": "N13", "properties": {{"outcome": "victory"}}}}
  ]
}}

## ì…ë ¥ í…ìŠ¤íŠ¸:
{synopsis}

## JSON ì‘ë‹µ:"""


def build_extraction_prompt(synopsis: str) -> str:
    """ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    valid_nodes_json = json.dumps(
        [{"id": k, "label": v["label"], "name": v["name"]} for k, v in VALID_NODES.items()],
        ensure_ascii=False, indent=2
    )
    valid_relationship_types = ", ".join(VALID_RELATIONSHIP_TYPES)
    
    return EXTRACTION_PROMPT_TEMPLATE.format(
        valid_nodes_json=valid_nodes_json,
        valid_relationship_types=valid_relationship_types,
        synopsis=synopsis
    )


def llm_call_structured(prompt: str, model: str = config.LLM_MODEL) -> GraphResponse:
    """êµ¬ì¡°í™”ëœ ì¶œë ¥ìœ¼ë¡œ OpenAI API í˜¸ì¶œ"""
    resp = client.beta.chat.completions.parse(
        model=model,
        messages=[
            {"role": "user", "content": prompt},
        ],
        response_format=GraphResponse,
    )
    return resp.choices[0].message.parsed


def normalize_node_id(node_id: str) -> Optional[str]:
    """ë…¸ë“œ ID ì •ê·œí™” - ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ None ë°˜í™˜"""
    # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    cleaned = re.sub(r'[^N0-9]', '', node_id)
    
    # N + ìˆ«ì í˜•ì‹ ì¶”ì¶œ
    match = re.match(r'(N\d+)', cleaned)
    if match:
        normalized = match.group(1)
        if normalized in VALID_NODES:
            return normalized
    return None


def normalize_relationship_type(rel_type: str) -> Optional[str]:
    """ê´€ê³„ íƒ€ì… ì •ê·œí™”"""
    # ëŒ€ë¬¸ìë¡œ ë³€í™˜í•˜ê³  ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    cleaned = re.sub(r'[^A-Z_]', '', rel_type.upper())
    
    # ìœ íš¨í•œ ê´€ê³„ íƒ€ì…ì´ë©´ ë°˜í™˜
    if cleaned in VALID_RELATIONSHIP_TYPES:
        return cleaned
    
    # ìœ ì‚¬í•œ íƒ€ì… ë§¤í•‘
    type_mapping = {
        "FIGHT": "FIGHTS",
        "BATTLE": "FIGHTS",
        "PROTECT": "PROTECTS",
        "TRAIN": "TRAINS",
        "KNOW": "KNOWS",
        "FAMILY": "FAMILY_OF",
        "ALLY": "ALLY_OF",
        "ENEMY": "ENEMY_OF",
        "DEFEAT": "DEFEATS",
        "SAVE": "SAVES",
        "MEET": "MEETS",
        "ATTACKS": "FIGHTS",
        "ATTACKED": "FIGHTS",
    }
    
    return type_mapping.get(cleaned, "FIGHTS")  # ê¸°ë³¸ê°’: FIGHTS


def validate_and_normalize_node(node: Node) -> Optional[Node]:
    """ë…¸ë“œ ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ê·œí™”"""
    # ID ì •ê·œí™”
    normalized_id = normalize_node_id(node.id)
    if not normalized_id:
        return None
    
    # ë§ˆìŠ¤í„° ë°ì´í„°ì—ì„œ ì˜¬ë°”ë¥¸ ê°’ ê°€ì ¸ì˜¤ê¸°
    master_node = VALID_NODES[normalized_id]
    
    return Node(
        id=normalized_id,
        label=master_node["label"],
        properties={"name": master_node["name"]}
    )


def validate_and_normalize_relationship(rel: Relationship) -> Optional[Relationship]:
    """ê´€ê³„ ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ê·œí™”"""
    # ë…¸ë“œ ID ì •ê·œí™”
    start_id = normalize_node_id(rel.start_node_id)
    end_id = normalize_node_id(rel.end_node_id)
    
    if not start_id or not end_id:
        return None
    
    # ê´€ê³„ íƒ€ì… ì •ê·œí™”
    rel_type = normalize_relationship_type(rel.type)
    
    return Relationship(
        type=rel_type,
        start_node_id=start_id,
        end_node_id=end_id,
        properties=rel.properties
    )


def validate_and_clean_graph(graph: GraphResponse) -> GraphResponse:
    """ê·¸ë˜í”„ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ì œ"""
    valid_nodes = []
    valid_relationships = []
    seen_node_ids = set()
    
    # ë…¸ë“œ ì •ì œ
    for node in graph.nodes:
        normalized = validate_and_normalize_node(node)
        if normalized and normalized.id not in seen_node_ids:
            valid_nodes.append(normalized)
            seen_node_ids.add(normalized.id)
    
    # ê´€ê³„ ì •ì œ
    for rel in graph.relationships:
        normalized = validate_and_normalize_relationship(rel)
        if normalized and normalized.start_node_id in VALID_NODES and normalized.end_node_id in VALID_NODES:
            valid_relationships.append(normalized)
    
    return GraphResponse(nodes=valid_nodes, relationships=valid_relationships)


def combine_chunk_graphs(chunk_graphs: List[GraphResponse]) -> GraphResponse:
    """ì—¬ëŸ¬ GraphResponseë¥¼ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤."""
    all_nodes = []
    all_relationships = []
    seen_nodes = set()
    seen_relationships = set()
    
    for chunk_graph in chunk_graphs:
        # ë…¸ë“œ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
        for node in chunk_graph.nodes:
            node_key = node.id
            if node_key not in seen_nodes:
                all_nodes.append(node)
                seen_nodes.add(node_key)
        
        # ê´€ê³„ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
        for rel in chunk_graph.relationships:
            rel_key = (rel.type, rel.start_node_id, rel.end_node_id, 
                      rel.properties.get("episode_number") if rel.properties else None)
            if rel_key not in seen_relationships:
                all_relationships.append(rel)
                seen_relationships.add(rel_key)
    
    return GraphResponse(nodes=all_nodes, relationships=all_relationships)


def fetch_episode(link: str) -> List[dict]:
    """ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ì—í”¼ì†Œë“œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤"""
    season = int(re.search(r"season_(\d+)", link).group(1))
    print(f"Fetching Season {season} from: {link}")
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    response = requests.get(link, headers=headers)
    
    soup = BeautifulSoup(response.text, "html.parser")
    table = soup.select_one("table.wikitable.plainrowheaders.wikiepisodetable")

    episodes = []
    rows = table.select("tr.vevent.module-episode-list-row")

    for i, row in enumerate(rows, start=1):
        synopsis = None
        synopsis_row = row.find_next_sibling("tr", class_="expand-child")
        if synopsis_row:
            synopsis_cell = synopsis_row.select_one("td.description div.shortSummaryText")
            synopsis = synopsis_cell.get_text(strip=True) if synopsis_cell else None

        episodes.append({
            "season": season,
            "episode_in_season": i,
            "synopsis": synopsis,
        })
    
    return episodes


def collect_data(use_cache: bool = True) -> List[dict]:
    """ì—¬ëŸ¬ ì‹œì¦Œì—ì„œ ì—í”¼ì†Œë“œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤"""
    print("=== ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ===")
    
    cache_file = "output/raw_data_v2.json"
    
    # ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¨¼ì € ì‚¬ìš©
    if use_cache and os.path.exists(cache_file):
        print(f"ìºì‹œ íŒŒì¼ ì‚¬ìš©: {cache_file}")
        with open(cache_file, "r", encoding="utf-8") as f:
            episodes = json.load(f)
        print(f"ì´ {len(episodes)}ê°œ ì—í”¼ì†Œë“œ ë¡œë“œ ì™„ë£Œ (ìºì‹œ)")
        return episodes
    
    episode_links = [
        "https://en.wikipedia.org/wiki/Demon_Slayer:_Kimetsu_no_Yaiba_season_1",
    ]
    
    all_episodes = []
    for link in episode_links:
        try:
            episodes = fetch_episode(link)
            all_episodes.extend(episodes)
        except Exception as e:
            print(f"Error fetching data from {link}: {e}")
            continue
    
    print(f"ì´ {len(all_episodes)}ê°œ ì—í”¼ì†Œë“œ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_episodes


def process_data(episodes: List[dict]) -> GraphResponse:
    """ì—í”¼ì†Œë“œ ë°ì´í„°ë¥¼ ì§€ì‹ ê·¸ë˜í”„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤"""
    print("=== ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ ===")
    
    chunk_graphs: List[GraphResponse] = []
    
    for episode in episodes:
        if not episode.get("synopsis"):
            print(f"ì—í”¼ì†Œë“œ S{episode['season']}E{episode['episode_in_season']:02d}: ì‹œë†‰ì‹œìŠ¤ê°€ ì—†ì–´ ê±´ë„ˆëœ€")
            continue
            
        print(f"ì—í”¼ì†Œë“œ ì²˜ë¦¬ ì¤‘: ì‹œì¦Œ {episode['season']}, ì—í”¼ì†Œë“œ {episode['episode_in_season']}")
        
        try:
            # (1) í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM í˜¸ì¶œ
            prompt = build_extraction_prompt(episode['synopsis'])
            graph_response = llm_call_structured(prompt)
            
            # (2) ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ì œ
            graph_response = validate_and_clean_graph(graph_response)

            # (3) ê´€ê³„ì— ì—í”¼ì†Œë“œ ë²ˆí˜¸ ì¶”ê°€
            episode_number = f"S{episode['season']}E{episode['episode_in_season']:02d}"
            for relationship in graph_response.relationships:
                if relationship.properties is None:
                    relationship.properties = {}
                relationship.properties["episode_number"] = episode_number
            
            chunk_graphs.append(graph_response)
            print(f"  - ì¶”ì¶œëœ ë…¸ë“œ: {len(graph_response.nodes)}, ê´€ê³„: {len(graph_response.relationships)}")
            
        except Exception as e:
            print(f"  - ì—í”¼ì†Œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    if not chunk_graphs:
        raise Exception("ê·¸ë˜í”„ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    print(f"ì´ {len(chunk_graphs)}ê°œ ì—í”¼ì†Œë“œ ì²˜ë¦¬ ì™„ë£Œ")
    return combine_chunk_graphs(chunk_graphs)


def save_output(episodes: List[dict], final_graph: GraphResponse):
    """ì¶œë ¥ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤"""
    print("=== ê²°ê³¼ ì €ì¥ ===")
    
    os.makedirs("output", exist_ok=True)
    
    # ì›ë³¸ ë°ì´í„° ì €ì¥
    with open("output/raw_data_v2.json", "w", encoding="utf-8") as f:
        json.dump(episodes, f, indent=2, ensure_ascii=False)
    print("ì›ë³¸ ë°ì´í„° ì €ì¥: output/raw_data_v2.json")
    
    # ìµœì¢… ì§€ì‹ ê·¸ë˜í”„ ì €ì¥
    with open("output/knowledge_graph_v2.json", "w", encoding="utf-8") as f:
        json.dump(final_graph.model_dump(), f, ensure_ascii=False, indent=2)
    print("ìµœì¢… ì§€ì‹ê·¸ë˜í”„ ì €ì¥: output/knowledge_graph_v2.json")


def validate_final_output():
    """ìµœì¢… ì¶œë ¥ íŒŒì¼ ê²€ì¦"""
    print("\n=== ìµœì¢… ì¶œë ¥ ê²€ì¦ ===")
    
    with open("output/knowledge_graph_v2.json", "r", encoding="utf-8") as f:
        data = json.load(f)
    
    errors = []
    
    # ë…¸ë“œ ê²€ì¦
    for node in data.get("nodes", []):
        if node["id"] not in VALID_NODES:
            errors.append(f"ì˜ëª»ëœ ë…¸ë“œ ID: {node['id']}")
        if node["label"] not in ["ì¸ê°„", "ë„ê¹¨ë¹„"]:
            errors.append(f"ì˜ëª»ëœ ë¼ë²¨: {node['label']} (ë…¸ë“œ: {node['id']})")
    
    # ê´€ê³„ ê²€ì¦
    for rel in data.get("relationships", []):
        if rel["start_node_id"] not in VALID_NODES:
            errors.append(f"ì˜ëª»ëœ ì‹œì‘ ë…¸ë“œ: {rel['start_node_id']}")
        if rel["end_node_id"] not in VALID_NODES:
            errors.append(f"ì˜ëª»ëœ ì¢…ë£Œ ë…¸ë“œ: {rel['end_node_id']}")
        if rel["type"] not in VALID_RELATIONSHIP_TYPES:
            errors.append(f"ì˜ëª»ëœ ê´€ê³„ íƒ€ì…: {rel['type']}")
    
    if errors:
        print("âŒ ê²€ì¦ ì‹¤íŒ¨:")
        for error in errors[:10]:
            print(f"  - {error}")
        if len(errors) > 10:
            print(f"  ... ì™¸ {len(errors) - 10}ê°œ ì˜¤ë¥˜")
        return False
    
    print("âœ… ê²€ì¦ í†µê³¼!")
    print(f"  - ìœ íš¨í•œ ë…¸ë“œ: {len(data.get('nodes', []))}ê°œ")
    print(f"  - ìœ íš¨í•œ ê´€ê³„: {len(data.get('relationships', []))}ê°œ")
    return True


def main():
    """ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¡°ìœ¨í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    try:
        print("ğŸš€ ì§€ì‹ê·¸ë˜í”„ ìƒì„±ê¸° ì‹œì‘")
        print("=" * 50)
        
        # ë‹¨ê³„ 1: ë°ì´í„° ìˆ˜ì§‘
        episodes = collect_data()
        
        if not episodes:
            raise Exception("ìˆ˜ì§‘ëœ ì—í”¼ì†Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë‹¨ê³„ 2: ë°ì´í„° ì²˜ë¦¬
        final_graph = process_data(episodes)
        
        # ë‹¨ê³„ 3: ì¶œë ¥ ì €ì¥
        save_output(episodes, final_graph)
        
        # ë‹¨ê³„ 4: ê²€ì¦
        validate_final_output()
        
        print("=" * 50)
        print("âœ… ì§€ì‹ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ë…¸ë“œ ìˆ˜: {len(final_graph.nodes)}")
        print(f"ğŸ”— ì´ ê´€ê³„ ìˆ˜: {len(final_graph.relationships)}")
        print("ìƒì„±ëœ íŒŒì¼:")
        print("- output/raw_data_v2.json")
        print("- output/knowledge_graph_v2.json")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
